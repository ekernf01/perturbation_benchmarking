## A systematic comparison of computational methods for expression forecasting ([preprint](https://www.biorxiv.org/content/10.1101/2023.07.28.551039v1))

This repo contains benchmark experiments to evaluate various strategies for predicting gene expression after knockout, knockdown, or overexpression. 

![image](https://github.com/ekernf01/perturbation_benchmarking/assets/5271803/ae7a5c86-dca6-49be-b048-743f8e110a18)

### Evaluate a new method

To evaluate a new method, make a [docker container with the behavior we expect](https://github.com/ekernf01/ggrn/tree/main/ggrn_docker_backend), then modify the [metadata for our docker demo experiment](https://github.com/ekernf01/perturbation_benchmarking/blob/main/experiments/ggrn_docker_backend/metadata.json), then run it:

```bash
cd perturbation_benchmarking
python do_one_experiment.py --experiment_name ggrn_docker_backend --amount_to_do missing_models --save_trainset_predictions \
    > experiments/ggrn_docker_backend/stdout.txt 2> experiments/ggrn_docker_backend/err.txt
```

Results are saved to `experiments/ggrn_docker_backend/outputs`. If it works, or doesn't work, please do get in touch by email or file a Github issue. We will be happy to hear from you!

### Install and/or reproduce results

This work involves a lot of dependencies. See `environment/install.md` for install instructions.

### Related infrastructure

This project is tightly coupled with our collections of data, our GGRN package for network inference, and a companion package containing benchmarking infrastructure. More information:

- Perturbation data, the network collection, and some accessory data (e.g. a list of TF's) are on Zenodo with DOI `10.5281/zenodo.8071809`.
    - Our code expects each of those three folders to be unzipped and placed adjacent to this repo.
    - Use our [perturbation loader](https://github.com/ekernf01/load_perturbations) and [network loader](https://github.com/ekernf01/load_networks) to easily access and validate data from Python.
- [GGRN](https://github.com/ekernf01/ggrn) offers flexible combination of different features for regulatory network inference.
- Our [perturbation benchmarking package](https://github.com/ekernf01/perturbation_benchmarking_package) helps conduct the `Experiment`s that are specified in this repo.
- Certain additional experiments are implemented in [our fork of DCD-FG](https://github.com/ekernf01/dcdfg).

### Usage

The benchmarks in this project are composed of small, structured folders called Experiments. Metadata fully describing each Experiment lives in a folder in `experiments`, and an individual Experiment can be run by calling `do_one_experiment.py`. For example, to run the experiment specified by `experiments/1.0_0/metadata.json`:

```bash
conda activate ggrn
python do_one_experiment.py --experiment_name 1.0_0 --amount_to_do missing_models --save_trainset_predictions \
    > experiments/1.0_0/stdout.txt 2> experiments/1.0_0/err.txt
```

The most important parts of each Experiment are:

- `outputs`: these results are auto-generated by our software based on user input and our data collections.
- `metadata.json`: A complete specification of the Experiment, provided by the user. 

Much more detail on metadata and outputs below.

It can be hard to understand how all our experiments relate to one another, software-wise or science-wise. Run `gather_experiment_metadata.py` to produce a quick summary table describing all experiments, or read `all_experiments.tsv` for the most recent such summary. 

#### Outputs

Here is an annotated layout of Experiment outputs.

```bash
├── conditions.csv # All combinations of values provided in the metadata.  
├── new_conditions.csv # This is generated and compared to any existing conditions.csv to prevent confusion upon editing metadata.
├── genes_modeled.csv # The genes included in this experiment.
├── mae.svg # Mean absolute prediction error for each test set observation
├── evaluationPerPert.parquet # Table of evaluation metrics listed separately for each observation in the test data, readable by e.g. pandas.read_parquet()
├── evaluationPerTarget.parquet # Table of evaluation metrics listed separately for each feature in the test data, readable by e.g. pandas.read_parquet()
├── targets 
│   ├── predictability_vs_in-degree.svg # Display of MAE of groups of targets stratified by in-degree in our networks.
│   ├── variety_in_predictions # Histogram meant to answer, "Are the predictions roughly constant?"
│   ├── enrichr_on_best # Enrichr pathway analysis of best-predicted targets for each condition in this experiment.
│   ├── best # Scatterplots of predicted vs observed for the best-predicted targets.
│   ├── random # Scatterplots of predicted vs observed for a few randomly chosen targets.
│   └── worst # Scatterplots of predicted vs observed for the worst-predicted targets.
├── perturbations # Same as targets but stratified by perturbation instead.
├── train_resources # Folder of CSV files with records of walltime and peak RAM consumption for each training run, one per row of conditions.csv.
├── fitted_values # Predictions on training data ...
│   ├── 0.h5ad # ... from row 0 of conditions.csv
│   ├── 1.h5ad # ... from row 1 of conditions.csv
│   ├── ...
├── predictions # Predictions on test data 
│   ├── 0.h5ad 
│   ├── 1.h5ad
│   ├── ...
└── trainset_performance # Same parquet files and plots as above, but for train-set
    ├── evaluationPerPert.parquet
    ├── evaluationPerTarget.parquet
```

Columns of `evaluationPerPert.parquet` and `evaluationPerTarget.parquet`:

- Evaluation metrics: `spearman spearmanp cell_type_correct mse_top_20 mse_top_100 mse_top_200 mse mae proportion_correct_direction`. `mae` is mean absolute error. `mse` is mean squared error. `mse_top_n` is the mean squared error on the n genes with highest test-set fold change. `proportion_correct_direction` is the proportion of genes going up (if they were predicted to go up) or down (if they were predicted to go down). `spearman`  is the Spearman correlation between the predicted log fold change and the observed log fold change, and `spearmanp` is the corresponding p-value. `cell_type_correct` is based on discrete training-set labels derived from predicted expression. `standard_deviation` measures not accuracy but rather how close the predictions are to just being constant. Many of these are only available per-pert or per-target and not both. `mae_baseline` and `mae_benefit` are for comparing each condition to a baseline but are now deprecated.
- Experimental conditions that match `metadata.json` and `conditions.csv` (more detail on these below): `perturbation index condition unique_id nickname question data_split_seed type_of_split regression_method num_genes eligible_regulators is_active facet_by color_by factor_varied merge_replicates perturbation_dataset network_datasets refers_to pruning_parameter pruning_strategy network_prior desired_heldout_fraction starting_expression feature_extraction control_subtype predict_self low_dimensional_structure low_dimensional_training matching_method prediction_timescale baseline_condition target mae_baseline mae_benefit`
- `gene`: the target gene (in `evaluationPerTarget.parquet`) or the perturbed gene (in `evaluationPerPert.parquet`) 
- Degree from our network collection: `in-degree_*` or `out-degree_*`
- Merged from per-gene metadata on transcript structure and mutation frequency: `transcript chr n_exons tx_start tx_end bp mu_syn mu_mis mu_lof n_syn n_mis n_lof exp_syn exp_mis exp_lof syn_z mis_z lof_z pLI n_cnv exp_cnv cnv_z`
- Expression characteristics of this gene in the training data: `highly_variable highly_variable_rank means variances variances_norm`
- Strength and reproducibility of the perturbation effects: `logFCNorm2 pearsonCorr spearmanCorr logFC`. Note: `logFC` refers to the targeted transcript only while `logFCNorm2` refers to global effect size. Only available in `evaluationPerPert.parquet`.

#### Metadata and designing an Experiment

For easy/immediate use, we recommend finding an Experiment similar to what you want to do and then editing it. Metadata will be validated by `perturbation_benchmarking_package.experimenter.validate_metadata()`, and this will provide useful error messages for simple issues such as required args that are missing. For more explicit documentation, read on. 

Experiment metadata files are JSON dictionaries. Most simple entries can be either a single value, or a list. If a list is provided, the experiment is run once for each item in the list. If multiple keys have lists, all combinations will be used. 

With apologies, many metadata keys have idiosyncratic formatting and meaning. 

- `perturbation_dataset` describes a dataset using the same names as our perturbation dataset collection. Only one dataset is allowed per Experiment. 
- `readme` describes the purpose of the experiment. 
- `nickname` conveys the essence curtly. 
- `unique_id` must match the folder the Experiment is in.
- `question` refers to `guiding_questions.txt` in this repo. 
- `is_active` must be `true` or the experiment won't run. 
- `skip_bad_runs`, if `true`, will allow an Experiment to continue if one condition encounters an error. Set this to `false` for easier debugging.
- `refers_to` points to another Experiment. If A refers to B, then all key/value pairs are copied from B's metadata unless explicitly provided in A's metadata. You may not refer to an experiment that already refers to something. You may not refer to multiple experiments.
- `kwargs` is a dict of keyword args passed on to GEARS, or DCD-FG, or any method [wrapped via Docker](https://github.com/ekernf01/ggrn_docker_backend). If you want to do a hyperparameter grid search, provide a list instead of a scalar, and use `kwargs_to_expand` to indicate that each element should be passed to your method separately.
- `data_split_seed`: integer setting the seed for repeatable data splits.
- `type_of_split`:     
    - if "interventional" (default), then any perturbation occurs in either the training or the test set, but not both. 
    - If "simple", then we use a simple random split, and replicates of the same perturbation are allowed to go into different folds or the same fold.
    - If "genetic_interaction", we put single perturbations and controls in the training set, and multiple perturbations in the test set.
    - If "demultiplexing", we put multiple perturbations and controls in the training set, and single perturbations in the test set.
    - If "stratified", we put some samples from each perturbation in the training set, and if there is replication, we put some in the test set. 
    - If "custom", we load the test set from the file 'custom_test_sets/<data_split_seed>.json'.
- `baseline_condition` DEPRECATED. This is a number, most often 0. This experimental condition, which corresponds to the same-numbered h5ad file in the `predictions` output and the same-numbered row in the `conditions.csv` output, is used as a baseline for computing performance improvement over baseline.
- `network_datasets` describes a GRN using the same names as our network collection. The behavior is complicated because the network collection separates out tissue-specific subnetworks. The value is a dict where keys are network sources and values are (sub-)dicts controlling specific behaviors.
    - To use certain subnetworks, set `subnets` to a list naming them. To use all, set subnets to all (default).
    - To take the union of the subnetworks, set `do_aggregate_subnets` to `true`. To keep subnetworks separate, set `do_aggregate_subnets` to `false` (default).
    - You can use `empty` or `dense` for a network with no edges or all possible edges. Default is `dense`. 
    - An example from experiment `1.3.2_9`:

            "network_datasets": {
                "empty": {},
                "dense": {},
                "magnum_compendium_394": {
                    "subnets": [
                        "retinal_pigment_epithelial_cells.parquet",
                        "chronic_myelogenous_leukemia_cml_cell_line.parquet",
                        "teratocarcinoma_cell_line.parquet",
                        "lung_adenocarcinoma_cell_line.parquet",
                        "breast_carcinoma_cell_line.parquet",
                        "embryonic_kidney_cell_line.parquet",
                        "hepatocellular_carcinoma_cell_line.parquet",
                        "epitheloid_cancer_cell_line.parquet",
                        "acute_myeloid_leukemia_fab_m5_cell_line.parquet"
                    ],
                    "do_aggregate_subnets": false
                }
            }
        

There are many other keys describing x/color/facet of the automated plots, the way network structures are used/pruned/ignored, and the regression methods used. Use `perturbation_benchmarking_package.experimenter.get_default_metadata()` to see the default values of each metadata field. Use `perturbation_benchmarking_package.experimenter.get_required_keys()` to see which keys are required.  Use `perturbation_benchmarking_package.experimenter.get_optional_keys()` to learn about optional keys.  
