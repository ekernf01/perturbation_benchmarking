	nickname	refers_to	readme
1.0_0	ml methods tiny	1.0_1	Like 1.0_1 but faster / smaller, mostly for realistic testing.
1.0_1	ml methods		We test a slate of regression methods to see if anything can beat ... the mean of the training data.
1.0_2	ml methods	1.0_1	Like 1.0_1 but replogle2 instead of nakatake.
1.0_3	ml methods	1.0_1	Like 1.0_1 but frangieh pseudobulk data instead of nakatake.
1.0_4	ml methods	1.0_1	Like 1.0_1 (various sklearn methods) but using only the GFP controls.
1.0_5	ml methods	1.0_1	Like 1.0_1 but replogle2_tf_only instead of nakatake.
1.0_6	ml methods	1.0_1	Like 1.0_1 but replogle2_large_effect instead of nakatake.
1.1.1_1	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.1.2_1	pruning_nakatake		This experiment compares models that use all genes as predictors versus models that only allow TF's to regulator other genes.
1.2.2_1	autoregressive		Do autoregressive models help extract more from the data than steady-state methods?
1.2.2_2	autoregressive	1.0_1	Hybrid of 1.2.2_1 and 1.0_1: autoregressive modeling of nakatake data.
1.2.2_3	autoregressive	1.0_1	Hybrid of 1.2.2_1 and 1.0_1: autoregressive modeling of nakatake data (1000 genes only).
1.3.1_1	CellTypeSpecificRegression		Q1.3 is about bias versus variance: does it work best to treat cell types as identical (high bias), separate (high variance), or similar (compromise)? This experiment and (sequels that refer to it) investigate the two extreme options by training one regression per cluster with either lots of clusters, or all data in one cluster. This experiment is currently not active and it may require some work to get it running again.
1.3.1_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_1	cellTypeSpecific		Do networks inferred for the cell type of interest work better than global networks or networks from the wrong cell types? Tested here with ANANSE cell-type-specific networks, ESC versus others, on the nakatake ESC perturbation data.
1.3.2_10	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_3	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_4	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_5	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_6	cellTypeSpecificFANTOM5Replogle1	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_7	cellTypeSpecificFANTOM5Replogle2	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_8	cellTypeSpecificFANTOM5Replogle3	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_9	cellTypeSpecificFANTOM5Replogle4	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.4.2_1	GEARS	1.0_1	Related Q asks, what's the best way of using a given network? GEARS has an interesting take on this. Here we test it out.
1.4.2_2	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_3	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_4	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_5	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_6	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.3_1	base_network		people have published big lists of TF-target or gene-gene relationships, often for GWAS interpretation or reprogramming. Existing benchmarks have limited information content and seldom compare these published network structures directly without introducing confounding factors. For instance, one might ask whether the networks used by CellNet, Mogrify, Irene, and CellOracle are of comparable value in predicting perturbation outcomes. Those methods have been compared, but they each involve many other components that may also affect the outcome, confounding the effect of network structure. This experiment benchmarks many networks using otherwise-equivalent methods to see how much each network helps predict held-out perturbations.
1.4.3_2	base_network	1.4.3_1	Network experiment but on replogle2
1.4.3_3	base_network	1.4.3_1	Network experiment but on frangieh_IFNÎ³_v3
1.4.3_4	base_network	1.4.3_1	Networks experiment but only using GFP controls.
1.4.3_5	base_network	1.4.3_1	Network experiment but on replogle2_tf_only
1.4.3_6	base_network	1.4.3_1	Network experiment but on replogle2_large_effect
1.4.4_1	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but based on 'replogle' dataset.
1.4.4_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.4.5_1	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_3, but based on 'replogle2' dataset.
1.4.6_1	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.4.7_1	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but based on the 'Frangieh' dataset.
1.6.1_1	dcdfg		Do DCD-FG and/or its variants beat simple baselines on their or our test data?
1.6.1_2	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but uses data preprocessed differently.
1.6.1_3	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but data preprocessed differently.
1.6.1_4	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but data preprocessed differently.
1.6.1_5	dcdfg	1.0_1	Compromise between experiment 1.6.1_1 (DCD-FG repro) and 1.0_1 (different ML methods on Nakatake).
1.6.1_6	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but with nakatake. Note: only 1000 genes selected.
1.8.4_0	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.8.5_0	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.8.6_0	blatantly cheating		Any autoregressive-like model needs a starting point to make predictions. Normally, one might use the control expression. Certain benchmarks, such as DCD-FG figure 5, instead feed in the heldout-data and allow each gene to be predicted as a function of heldout expression of other genes. This is necessary to e.g. compute the DCD-FG likelihood. How much difference does this make?
1.9_0	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on the celloracle network.
1.9_1	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on the celloracle network. Unlike 1.9_0, data are NOT at steady state.
1.9_2	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on the celloracle network. Unlike 1.9_0, data are NOT at steady state.
1.9_3	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on the celloracle network. Unlike 1.9_0, data are NOT at steady state.
test	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
