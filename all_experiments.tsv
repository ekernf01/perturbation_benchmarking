	nickname	refers_to	readme
.ipynb_checkpoints	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.0_0	ml methods tiny	1.0_1	Like 1.0_1 but faster / smaller, mostly for realistic testing.
1.0_1	ml methods		We test a slate of regression methods to see if anything can beat ... the mean of the training data.
1.0_2	ml methods	1.0_1	We test a slate of regression methods to see if anything can beat ... the mean of the training data.
1.1.1_1	pruning_nakatake		Some models use sparse network structures and some dense. Though the true causal structures are expected to be sparse, some preliminary results found that restrictive sparse structures (pre-specified by the user) are not competitive in terms of predicting held-out perturbations. This experiment compares denser and sparse models in terms of heldout performance.
1.1.2_1	pruning_nakatake		This experiment compares models that use all genes as predictors versus models that only allow TF's to regulator other genes.
1.3.1_1	CellTypeSpecificRegression		Q1.3 is about bias versus variance: does it work best to treat cell types as identical (high bias), separate (high variance), or similar (compromise)? This experiment and (sequels that refer to it) investigate the two extreme options by training one regression per cluster with either lots of clusters, or all data in one cluster. This experiment is currently not active and it may require some work to get it running again.
1.3.1_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_1	cellTypeSpecific		Do networks inferred for the cell type of interest work better than global networks or networks from the wrong cell types? Tested here with ANANSE cell-type-specific networks, ESC versus others, on the nakatake ESC perturbation data.
1.3.2_10	cellTypeSpecificFANTOM5CMAP	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_2	cellTypeSpecificCellNetHg1332	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_3	cellTypeSpecificCellNetHugene	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_4	cellTypeSpecificANANSE	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_5	cellTypeSpecificCSNets	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_6	cellTypeSpecificFANTOM5Replogle1	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_7	cellTypeSpecificFANTOM5Replogle2	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_8	cellTypeSpecificFANTOM5Replogle3	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_9	cellTypeSpecificFANTOM5Replogle4	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.4.3_1	base_network		people have published big fat lists of TF-target or gene-gene relationships, often for GWAS interpretation or reprogramming. Existing benchmarks have limited information content and seldom compare these published network structures directly without introducing confounding factors. For instance, one might ask whether the networks used by CellNet, Mogrify, Irene, and CellOracle are of comparable value in predicting perturbation outcomes. Those methods have been compared, but they each involve many other components that may also affect the outcome, confounding the effect of network structure. This experiment benchmarks many networks using otherwise-equivalent methods to see how much each network helps predict held-out perturbations.
1.4.3_2	base_network	1.4.3_1	This experiment uses the same logic as experiment <refers_to>, but with replicates merged.
1.4.3_3	base_network	1.4.3_1	This is like experiment 1.4.3_2, except we simulate all target gene expression from the 'gtex_rna' network structure.
1.4.3_4	base_network	1.4.3_1	This is like experiment 1.4.3_3, except we use more conservative penalty param selection.
1.4.3_5	base_network	1.4.3_1	This is like experiment 1.4.3_2, except we use more conservative penalty param selection.
1.4.3_6	base_network	1.4.3_1	This is like experiment 1.4.3_2, except we use more conservative penalty param selection and we use the 'two-step' time strategy.
1.4.4_1	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but based on 'replogle' dataset.
1.4.4_2	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_3, but simulations based on 'replogle' dataset.
1.4.5_1	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_3, but based on 'replogle2' dataset.
1.4.5_2	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_3, but simulations based on 'replogle2' dataset.
1.4.6_1	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but based on 'replogle3' dataset.
1.4.6_2	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_3, but simulations based on 'replogle3' dataset.
1.6.1_1	dcdfg		Do DCD-FG and/or its variants beat simple baselines on their or our test data?
1.6.1_2	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but uses data preprocessed differently.
1.6.1_3	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but data preprocessed differently.
1.8.4_0	data split		This is intended to test whether the problem actually gets harder when we split the data 'unevenly', with no common perturbations between train and test data.
1.8.5_0	data split		This is intended to test how different data splits affect apparent performance.
1.8.6_0	blatantly cheating		Any autoregressive-like model needs a starting point to make predictions. Normally, one might use the control expression. Certain benchmarks, such as DCD-FG figure 5, instead feed in the heldout-data and allow each gene to be predicted as a function of heldout expression of other genes. This is necessary to e.g. compute the DCD-FG likelihood. How much difference does this make?
template	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
template_refers_to	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
test	test		This experiment is a sandbox meant to test new features of the benchmarking code.
