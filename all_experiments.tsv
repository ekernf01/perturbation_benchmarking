	nickname	refers_to	readme
1.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.0_0	ml methods tiny	1.0_1	Like 1.0_1 but faster / smaller, mostly for realistic testing.
1.0_1	ml methods		We test a slate of regression methods to see if anything can beat ... the mean of the training data.
1.0_10	ml methods	1.0_1	Like 1.0_1 but  instead of nakatake.
1.0_11	ml methods	1.0_1	Like 1.0_1 but different data instead of nakatake.
1.0_12	ml methods	1.0_1	Like 1.0_1 but different data instead of nakatake.
1.0_13	ml methods	1.0_1	Like 1.0_1 but different data instead of nakatake.
1.0_2	ml methods	1.0_1	Like 1.0_1 but replogle2 instead of nakatake.
1.0_3	ml methods	1.0_1	Like 1.0_1 but frangieh pseudobulk data instead of nakatake.
1.0_4	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.0_5	ml methods	1.0_1	Like 1.0_1 but replogle2_tf_only instead of nakatake.
1.0_6	ml methods	1.0_1	Like 1.0_1 but replogle2_large_effect instead of nakatake.
1.0_7	ml methods	1.0_1	Like 1.0_1 but freimer instead of nakatake.
1.0_8	ml methods	1.0_1	Like 1.0_1 but replogle instead of nakatake.
1.0_9	ml methods	1.0_1	Like 1.0_1 but replogle4 instead of nakatake.
1.1.1_1	hyperparam_sweep		This is a simple sweep over the regularization parameter for LASSO regression.
1.1.2_1	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.2.2_0	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.2.2_1	matching_and_timescale		Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_10	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_11	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_12	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_13	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_14	matching_and_timescale		Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_15	matching_and_timescale	1.2.2_14	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_16	matching_and_timescale	1.2.2_14	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_17	matching_and_timescale	1.2.2_14	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_18	matching_and_timescale	1.2.2_14	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_19	matching_and_timescale	1.2.2_14	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_2	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_20	matching_and_timescale	1.2.2_14	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_21	matching_and_timescale	1.2.2_14	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_22	matching_and_timescale	1.2.2_14	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_3	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_5	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_6	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_7	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_8	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_9	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.3.1_1	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.1_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_1	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_10	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_3	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_4	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_5	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_6	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_7	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_8	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_9	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.3_1	TransferLearning		Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_10	TransferLearning	1.3.3_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_2	TransferLearning	1.3.3_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_3	TransferLearning	1.3.3_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_5	TransferLearning	1.3.3_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_6	TransferLearning	1.3.3_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_7	TransferLearning	1.3.3_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_8	TransferLearning	1.3.3_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_9	TransferLearning	1.3.3_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.4.1_0	timeseries celltype networks		Comparing cell type specific versus universal networks for timeseries prediction
1.4.1_1	timeseries celltype networks	1.4.1_0	Comparing cell type specific versus universal networks for timeseries prediction
1.4.1_2	timeseries celltype networks	1.4.1_0	Comparing cell type specific versus universal networks for timeseries prediction
1.4.1_3	timeseries celltype networks	1.4.1_0	Comparing cell type specific versus universal networks for timeseries prediction
1.4.1_4	timeseries celltype networks	1.4.1_0	Comparing cell type specific versus universal networks for timeseries prediction
1.4.2_1	GEARS		Related Q asks, what's the best way of using a given network? GEARS has an interesting take on this. Here we test it out.
1.4.2_10	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.4.2_11	GEARS	1.4.2_1	Test of GEARS on other datasets.
1.4.2_12	GEARS	1.4.2_1	Test of GEARS on other datasets.
1.4.2_13	GEARS	1.4.2_1	Test of GEARS on other datasets.
1.4.2_14	GEARS	1.4.2_1	Test of GEARS on other datasets.
1.4.2_2	GEARS	1.4.2_1	Test of GEARS on their preferred demo datasets.
1.4.2_3	GEARS	1.4.2_1	Test of GEARS on their preferred demo datasets.
1.4.2_4	GEARS	1.4.2_1	Test of GEARS on their preferred demo datasets.
1.4.2_5	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.4.2_6	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.4.2_8	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.4.3_1	base_network		people have published big lists of TF-target or gene-gene relationships, often for GWAS interpretation or reprogramming. Existing benchmarks have limited information content and seldom compare these published network structures directly without introducing confounding factors. For instance, one might ask whether the networks used by CellNet, Mogrify, Irene, and CellOracle are of comparable value in predicting perturbation outcomes. Those methods have been compared, but they each involve many other components that may also affect the outcome, confounding the effect of network structure. This experiment benchmarks many networks using otherwise-equivalent methods to see how much each network helps predict held-out perturbations.
1.4.3_10	base_network	1.4.3_1	Network experiment but on replogle3
1.4.3_11	base_network	1.4.3_1	Network experiment but on nakatake_simulated_scrna
1.4.3_12	base_network	1.4.3_1	Network experiment but on joung
1.4.3_13	base_network	1.4.3_1	Network experiment but on a different dataset
1.4.3_2	base_network	1.4.3_1	Network experiment but on replogle2
1.4.3_3	base_network	1.4.3_1	Network experiment but on frangieh_IFNg_v3
1.4.3_4	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.4.3_5	base_network	1.4.3_1	Network experiment but on replogle2_tf_only
1.4.3_6	base_network	1.4.3_1	Network experiment but on replogle2_large_effect
1.4.3_7	base_network	1.4.3_1	Network experiment but on freimer
1.4.3_8	base_network	1.4.3_1	Network experiment but on replogle1
1.4.3_9	base_network	1.4.3_1	Network experiment but on replogle4
1.4.4_1	network_only		Are network-connected genes enriched for perturbation responses? This experiment uses network structure alone for prediction, with no training data and all perturbations reserved for evaluation.
1.4.4_2	network_only		Are network-connected genes enriched for perturbation responses? This experiment uses network structure alone for prediction, with no training data and all perturbations reserved for evaluation.
1.4.4_3	network_only	1.4.4_1	Are network-connected genes enriched for perturbation responses? This experiment uses network structure alone for prediction, with no training data and all perturbations reserved for evaluation.
1.4.4_4	network_only	1.4.4_2	Are network-connected genes enriched for perturbation responses? This experiment uses network structure alone for prediction, with no training data and all perturbations reserved for evaluation.
1.4.4_5	network_only	1.4.4_1	Are network-connected genes enriched for perturbation responses? This experiment uses network structure alone for prediction, with no training data and all perturbations reserved for evaluation.
1.4.4_6	network_only	1.4.4_2	Are network-connected genes enriched for perturbation responses? This experiment uses network structure alone for prediction, with no training data and all perturbations reserved for evaluation.
1.4.4_7	network_only	1.4.4_1	Are network-connected genes enriched for perturbation responses? This experiment uses network structure alone for prediction, with no training data and all perturbations reserved for evaluation.
1.4.4_8	network_only	1.4.4_2	Are network-connected genes enriched for perturbation responses? This experiment uses network structure alone for prediction, with no training data and all perturbations reserved for evaluation.
1.4.5_1	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.5.1_0	timeseries pilot		Direct comparison of published timeseries methods
1.5.1_1	timeseries pilot	1.5.1_0	Direct comparison of published timeseries methods
1.5.1_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.5.1_3	timeseries pilot	1.5.1_0	Direct comparison of published timeseries methods
1.5.1_4	timeseries pilot	1.5.1_0	Direct comparison of published timeseries methods
1.5.1_5	timeseries pilot	1.5.1_0	Direct comparison of published timeseries methods
1.5.1_6	timeseries pilot	1.5.1_0	Direct comparison of published timeseries methods
1.5.1_7	timeseries pilot	1.5.1_0	Direct comparison of published timeseries methods
1.5.2_0	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.6.1_1	dcdfg		Comparison of several published methods (originally focused on DCD-FG, hence the numbering after Q1.6).
1.6.1_10	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_11	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_12	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_13	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_14	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_15	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_16	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_17	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_18	dcdfg	1.6.1_1	Parameter sweep for a less sparse NO-TEARS model.
1.6.1_19	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.6.1_2	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but uses data preprocessed differently.
1.6.1_3	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but data preprocessed differently.
1.6.1_4	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data.
1.6.1_6	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but with nakatake. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_7	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_8	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_9	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.3_0	timeseries low rank		Comparison of linear models with full-rank and low-rank transition matrices
1.6.3_1	timeseries pilot	1.6.3_0	Comparison of linear models with full-rank and low-rank transition matrices
1.6.3_2	timeseries pilot	1.6.3_0	Comparison of linear models with full-rank and low-rank transition matrices
1.6.3_3	timeseries pilot	1.6.3_0	Comparison of linear models with full-rank and low-rank transition matrices
1.6.3_4	timeseries pilot	1.6.3_0	Comparison of linear models with full-rank and low-rank transition matrices
1.6.3_5	timeseries pilot	1.6.3_0	Comparison of linear models with full-rank and low-rank transition matrices
1.6.3_6	timeseries pilot	1.6.3_0	Comparison of linear models with full-rank and low-rank transition matrices
1.6.3_7	timeseries pilot	1.6.3_0	Comparison of linear models with full-rank and low-rank transition matrices
1.6.3_8	timeseries pilot	1.6.3_0	Comparison of linear models with full-rank and low-rank transition matrices
1.9_0	base_network_simulation	1.9_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known networks.
1.9_1	base_network_simulation		Testing different network structures like in experiment 1.4.3_1, but with simulated data based on a known network. 
1.9_2	base_network_simulation	1.9_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known network.
1.9_3	base_network_simulation	1.9_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known network. 
1.9_4	base_network_simulation	1.9_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known network.
5_0	scaling		How do different methods scale in practice?
ggrn_docker_backend	ggrn_docker_backend		Test of the ggrn backend that runs a user-specified docker container.
ggrn_docker_backend_celloracle	ggrn_docker_backend_celloracle		Test of the ggrn backend that runs a docker container with CO installed.
singularity_demo	singularity_demo		Trying to get backends working with singularity
test	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
