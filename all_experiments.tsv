	nickname	refers_to	readme
1.0_0	ml methods tiny	1.0_1	Like 1.0_1 but faster / smaller, mostly for realistic testing.
1.0_1	ml methods		We test a slate of regression methods to see if anything can beat ... the mean of the training data.
1.0_10	ml methods	1.0_1	Like 1.0_1 but  instead of nakatake.
1.0_2	ml methods	1.0_1	Like 1.0_1 but replogle2 instead of nakatake.
1.0_3	ml methods	1.0_1	Like 1.0_1 but frangieh pseudobulk data instead of nakatake.
1.0_4	ml methods	1.0_1	Like 1.0_1 (various sklearn methods) but using only the GFP controls.
1.0_5	ml methods	1.0_1	Like 1.0_1 but replogle2_tf_only instead of nakatake.
1.0_6	ml methods	1.0_1	Like 1.0_1 but replogle2_large_effect instead of nakatake.
1.0_7	ml methods	1.0_1	Like 1.0_1 but freimer instead of nakatake.
1.0_8	ml methods	1.0_1	Like 1.0_1 but replogle instead of nakatake.
1.0_9	ml methods	1.0_1	Like 1.0_1 but replogle4 instead of nakatake.
1.1.1_1	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.1.2_1	pruning_nakatake		This experiment compares models that use all genes as predictors versus models that only allow TF's to regulator other genes.
1.2.2_1	matching_and_timescale		Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_10	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_2	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_3	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_5	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_6	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_7	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_8	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.2.2_9	matching_and_timescale	1.2.2_1	Testing whether we should use the steady-state assumption or match to controls, and whether we should predict after one, a few, or many time-steps.
1.3.1_1	CellTypeSpecificRegression		Q1.3 is about bias versus variance: does it work best to treat cell types as identical (high bias), separate (high variance), or similar (compromise)? This experiment and (sequels that refer to it) investigate the two extreme options by training one regression per cluster with either lots of clusters, or all data in one cluster. This experiment is currently not active and it may require some work to get it running again.
1.3.1_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_1	cellTypeSpecific		Do networks inferred for the cell type of interest work better than global networks or networks from the wrong cell types? Tested here with ANANSE cell-type-specific networks, ESC versus others, on the nakatake ESC perturbation data.
1.3.2_10	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_2	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_3	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_4	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_5	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.3.2_6	cellTypeSpecificFANTOM5Replogle1	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_7	cellTypeSpecificFANTOM5Replogle2	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_8	cellTypeSpecificFANTOM5Replogle3	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.2_9	cellTypeSpecificFANTOM5Replogle4	1.3.2_1	This experiment uses the same logic as experiment <refers_to>, with different networks and perturbation data.
1.3.3_1	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_10	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_2	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_3	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_5	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_6	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_7	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_8	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.3.3_9	TransferLearning	1.0_1	Q1.3.3 is about whether we can learn causal effects by pretraining a big fat transformer on a big fat collection of scRNA data.
1.4.2_1	GEARS	1.0_1	Related Q asks, what's the best way of using a given network? GEARS has an interesting take on this. Here we test it out.
1.4.2_10	GEARS	1.0_1	Test of GEARS on other datasets.
1.4.2_11	GEARS	1.0_1	Test of GEARS on other datasets.
1.4.2_12	GEARS	1.0_1	Test of GEARS on other datasets.
1.4.2_2	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_3	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_4	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_5	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_6	GEARS	1.0_1	Test of GEARS on their preferred demo datasets.
1.4.2_7	GEARS	1.0_1	Test of GEARS on other datasets.
1.4.2_8	GEARS	1.0_1	Test of GEARS on other datasets.
1.4.2_9	GEARS	1.0_1	Test of GEARS on other datasets.
1.4.3_1	base_network		people have published big lists of TF-target or gene-gene relationships, often for GWAS interpretation or reprogramming. Existing benchmarks have limited information content and seldom compare these published network structures directly without introducing confounding factors. For instance, one might ask whether the networks used by CellNet, Mogrify, Irene, and CellOracle are of comparable value in predicting perturbation outcomes. Those methods have been compared, but they each involve many other components that may also affect the outcome, confounding the effect of network structure. This experiment benchmarks many networks using otherwise-equivalent methods to see how much each network helps predict held-out perturbations.
1.4.3_10	base_network	1.4.3_1	Network experiment but on replogle3
1.4.3_2	base_network	1.4.3_1	Network experiment but on replogle2
1.4.3_3	base_network	1.4.3_1	Network experiment but on frangieh_IFNÎ³_v3
1.4.3_4	base_network	1.4.3_1	Networks experiment but only using GFP controls.
1.4.3_5	base_network	1.4.3_1	Network experiment but on replogle2_tf_only
1.4.3_6	base_network	1.4.3_1	Network experiment but on replogle2_large_effect
1.4.3_7	base_network	1.4.3_1	Network experiment but on freimer
1.4.3_8	base_network	1.4.3_1	Network experiment but on replogle
1.4.3_9	base_network	1.4.3_1	Network experiment but on replogle4
1.6.1_1	dcdfg		Do DCD-FG and/or its variants beat simple baselines on their or our test data?
1.6.1_10	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_11	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_2	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but uses data preprocessed differently.
1.6.1_3	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but data preprocessed differently.
1.6.1_4	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but data preprocessed differently.
1.6.1_5	dcdfg	1.0_1	Compromise between experiment 1.6.1_1 (DCD-FG repro) and 1.0_1 (different ML methods on Nakatake). This may be unworkable as-is due to the high number of features.
1.6.1_6	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but with nakatake. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_7	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_8	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.6.1_9	dcdfg	1.6.1_1	Like experiment 1.6.1_1, but different data and starting_expression. Note: only 1000 genes selected, as in 1.6.1_1.
1.8.4_0	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.8.5_0	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
1.8.6_0	blatantly cheating		Any autoregressive-like model needs a starting point to make predictions. Normally, one might use the control expression. Certain benchmarks, such as DCD-FG figure 5, instead feed in the heldout-data and allow each gene to be predicted as a function of heldout expression of other genes. This is necessary to e.g. compute the DCD-FG likelihood. How much difference does this make?
1.9_0	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known networks.
1.9_1	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known network. 
1.9_2	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known network.
1.9_3	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known network. 
1.9_4	base_network	1.4.3_1	Testing different network structures like in experiment 1.4.3_1, but with simulated data based on known network.
ggrn_docker_backend	ggrn_docker_backend		Test of the ggrn backend that runs a user-specified docker container.
test	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.	Could not validate the metadata -- likely an inactive experiment.
